{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO1c4k76fEQbbt8lpVEtPNP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samiravaez/medical-text-data-classifier/blob/main/MedicalTextDeepClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MLMbngyBUUvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoDoG6IN1aA7",
        "outputId": "08c51f44-36f9-4bc0-fe87-1d87ff6d87ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('drive')\n",
        "import tensorflow as tf\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Dropout, Conv1D, MaxPool1D, GlobalMaxPool1D, Embedding, Activation\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "import re\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "\n",
        "target_names = ['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21','22','23']\n",
        "\n",
        "def get_files_in_dir(directory):\n",
        "    file_names_list = []\n",
        "    for file_name in glob.glob(directory):\n",
        "        file_names_list += [file_name]\n",
        "    return file_names_list\n",
        "\n",
        "\n",
        "def get_text_file_data(file):\n",
        "    data = []\n",
        "    with open(file, 'r', encoding=\"latin-1\") as file:\n",
        "      for line in file:\n",
        "        data += [str(line)]\n",
        "    return data\n",
        "\n",
        "\n",
        "content = []\n",
        "labels = []\n",
        "\n",
        "for i in range(1,24): #this should be 24\n",
        "  p = 'C'+'%.2d' % i\n",
        "  file_path = os.path.join('drive/MyDrive/ohsumed-all',str(p),'*')\n",
        "  for file in get_files_in_dir(file_path):\n",
        "     file_data = get_text_file_data(file)\n",
        "     content += [\"\".join(file_data)]\n",
        "     labels += [i]\n",
        "\n",
        "\n",
        "\n",
        "# word2vec vectorization\n",
        "features_data = pd.DataFrame(data=content, columns=['medText'])\n",
        "new_data = features_data.medText.apply(gensim.utils.simple_preprocess)\n",
        "# word2vec_model = gensim.models.Word2Vec(sentences=new_data, vector_size=100, window=5, sg=1)\n",
        "# word2vec_model.save(\"word2vec.model\")\n",
        "\n",
        "#### loading the word2vec model\n",
        "word2vec_model = gensim.models.Word2Vec.load('drive/MyDrive/word2vec.model')\n",
        "word_vectors = word2vec_model.wv.vectors\n",
        "\n",
        "# data pre-process\n",
        "max_sequence_length = 100\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(content)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(content)\n",
        "\n",
        "# Padding sequences to have a consistent length\n",
        "data = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "\n",
        "#split data\n",
        "labels = np.array(labels)\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.35, random_state=10)\n",
        "\n",
        "# defining one-hot encoding for multi-class scenario\n",
        "labels_onehot = to_categorical(labels_train, num_classes=len(target_names))\n",
        "labels_test_onehot = to_categorical(labels_test, num_classes=len(target_names))\n",
        "\n",
        "\n",
        "# Define the CNN model\n",
        "keras_model = Sequential()\n",
        "keras_model.add(Embedding(\n",
        "    input_dim=word_vectors.shape[0],\n",
        "    output_dim=word_vectors.shape[1],\n",
        "    input_length=max_sequence_length,\n",
        "    weights=[word_vectors],\n",
        "    trainable=False\n",
        "))\n",
        "keras_model.add(Conv1D(128, 5, activation='relu', padding='same'))\n",
        "keras_model.add(Conv1D(128, 5, activation='relu', padding='same'))\n",
        "keras_model.add(MaxPool1D())\n",
        "keras_model.add(Conv1D(200, 5, activation='relu', padding='same'))\n",
        "keras_model.add(Conv1D(200, 5, activation='relu', padding='same'))\n",
        "keras_model.add(GlobalMaxPool1D())\n",
        "keras_model.add(Dense(64, activation='relu'))\n",
        "keras_model.add(Dropout(0.2))\n",
        "keras_model.add(Dense(len(target_names), activation='softmax'))\n",
        "\n",
        "\n",
        "keras_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "keras_model.fit(features_train, labels_onehot, epochs=5, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "loss, accuracy = keras_model.evaluate(features_test, labels_test_onehot)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n"
      ]
    }
  ]
}