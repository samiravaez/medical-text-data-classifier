{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeaBhaPGI2++6QnFVKlNnc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samiravaez/medical-text-data-classifier/blob/main/MedicalTextDeepClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoDoG6IN1aA7",
        "outputId": "baa602da-7821-415d-ad85-8e1818620e63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import gensim\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Dropout, Conv1D, MaxPool1D, GlobalMaxPool1D, Embedding, Activation\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "import re\n",
        "from sklearn import preprocessing\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "target_names = ['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21','22','23']\n",
        "\n",
        "def get_files_in_dir(directory):\n",
        "    file_names_list = []\n",
        "    for file_name in glob.glob(directory):\n",
        "        file_names_list += [file_name]\n",
        "    return file_names_list\n",
        "\n",
        "\n",
        "def get_text_file_data(file):\n",
        "    data = []\n",
        "    with open(file, 'r', encoding=\"latin-1\") as file:\n",
        "      for line in file:\n",
        "        data += [str(line)]\n",
        "    return data\n",
        "\n",
        "\n",
        "content = []\n",
        "labels = []\n",
        "\n",
        "for i in range(1,24): #this should be 24\n",
        "  p = 'C'+'%.2d' % i\n",
        "  file_path = os.path.join('drive/MyDrive/ohsumed-all',str(p),'*')\n",
        "  for file in get_files_in_dir(file_path):\n",
        "     file_data = get_text_file_data(file)\n",
        "     content += [\"\".join(file_data)]\n",
        "     labels += [i]\n",
        "\n",
        "\n",
        "\n",
        "# word2vec vectorization\n",
        "features_data = pd.DataFrame(data=content, columns=['medText'])\n",
        "new_data = features_data.medText.apply(gensim.utils.simple_preprocess)\n",
        "# word2vec_model = gensim.models.Word2Vec(sentences=new_data, vector_size=100, window=5, sg=1)\n",
        "# word2vec_model.save(\"word2vec.model\")\n",
        "\n",
        "#### loading the word2vec model\n",
        "word2vec_model = gensim.models.Word2Vec.load('drive/MyDrive/word2vec.model')\n",
        "max_sequence_length = 100  # You can adjust this based on your data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(content)\n",
        "sequences = tokenizer.texts_to_sequences(content)\n",
        "\n",
        "# Padding sequences to have a consistent length\n",
        "data = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "content = np.array(content)\n",
        "labels = np.array(labels)\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(content, labels, test_size=0.35, random_state=10)\n",
        "\n",
        "\n",
        "# Define the CNN model\n",
        "keras_model = Sequential()\n",
        "keras_model.add(word2vec_model.wv.get_keras_embedding(True))\n",
        "keras_model.add(Dropout(0.2))\n",
        "keras_model.add(Conv1D(128, 5, activation='relu', padding='same'))\n",
        "keras_model.add(Conv1D(128, 5, activation='relu', padding='same'))\n",
        "keras_model.add(MaxPool1D())\n",
        "keras_model.add(Conv1D(200, 5, activation='relu', padding='same'))\n",
        "keras_model.add(Conv1D(200, 5, activation='relu', padding='same'))\n",
        "keras_model.add(GlobalMaxPooling1D())\n",
        "keras_model.add(Dense(64, activation='relu'))\n",
        "keras_model.add(Dropout(0.2))\n",
        "keras_model.add(Dense(23, activation='sigmoid'))\n",
        "\n",
        "\n",
        "keras_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "keras_model.fit(features_train, labels_train, epochs=5, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "loss, accuracy = keras_model.evaluate(features_test, labels_test)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n"
      ]
    }
  ]
}
